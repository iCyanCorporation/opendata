# opendata

> \*\*Collect, Structure, and Sh### ğŸ§  Design Principles

- **Modular**: One YAML configuration file per topic-country pair
- **Scalable**: Easy to add new topics or countries by adding new configuration files
- **Lightweight**: Files capped at 100MB to stay within GitHub's limits
- **Auditable**: Every change is tracked with version control
- **Centralized Config**: Country codes and other settings live in a shared `config/` folder
- **Automated**: GitHub Actions handle all scheduled updates
- **Declarative**: Data sources defined in YAML rather than code, making it easier to add new sources Web Data via GitHub Actions\*\*

## ğŸ“Œ Project Overview

**opendata** is an automated, topic- and country-aware data pipeline that **crawls public data sources** (HTML, PDFs, Excel, CSV, etc.) from the internet and saves them in **CSV files categorized by topic and country** under a GitHub-hosted repository. All data is refreshed and published using GitHub Actions.

### ğŸ¯ Purpose

- Centralize public open data in a consistent and reusable CSV format.
- Support cross-country comparisons by categorizing data per topic and per country.
- Automate collection and updates via scheduled GitHub Actions.

### ğŸš¨ Problem Being Solved

Public data is fragmented across formats, languages, and domains. This project **unifies** and **standardizes** datasets, making them programmatically accessible and version-controlled.

### ğŸ‘¥ Target Audience

- Data scientists, researchers, and educators
- Developers building open-data tools
- Policy analysts, journalists, and the public
- Open-source contributors interested in data engineering

---

## ğŸ— Architecture & Design Principles

### ğŸ§± Tech Stack

- **Language**: Python 3.11+
- **Automation**: GitHub Actions
- **Parsing**:

  - HTML: `requests`, `BeautifulSoup4`
  - PDF: `pdfplumber`, `PyMuPDF`
  - Excel: `pandas`, `openpyxl`, `xlrd`
  - CSV: `pandas`

- **Output Format**: `.csv` files under `data/`, named as `{topic}_{countryCode}.csv`

### ğŸ§  Design Principles

- **Modular**: One crawler per topic-country pair
- **Scalable**: Easy to add new topics or countries
- **Lightweight**: Files capped at 100MB to stay within GitHubâ€™s limits
- **Auditable**: Every change is tracked with version control
- **Centralized Config**: Country codes and other settings live in a shared `config/` folder
- **Automated**: GitHub Actions handle all scheduled updates

---

## âš™ï¸ Installation & Setup

### ğŸ”§ Prerequisites

- Python 3.11+
- Git
- (Optional) Virtual environment: `venv` or `poetry`

### ğŸ“¦ Setup Instructions

```bash
git clone https://github.com/<your-org>/opendata.git
cd opendata
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### âš™ï¸ Environment Variables

If needed, create a `.env` file:

```ini
API_KEY=your_api_key_here
```

Use `python-dotenv` to load it automatically in your scripts.

---

## ğŸš€ Usage Guidelines

### ğŸ”¹ Run a Specific Topic-Country Crawler (Manually)

```bash
python crawl_all.py --topic health --country us
```

â†’ Generates `data/health/2025/05/21/us.csv`

### ğŸ”„ Crawl All Topics for All Countries

```bash
python crawl_all.py
```

This script will:

- Read supported country codes from `config/countries.yaml`
- Discover all topic folders and matching YAML configuration files
- Run the appropriate crawler based on the source type specified in the YAML file
- Write output to `data/{topic}/{yyyy}/{mm}/{dd}/{countryCode}.csv`

### ğŸ§ª Run Tests

```bash
pytest tests/
```

---

## ğŸ—‚ Code & Folder Structure

```
opendata/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ countries.yaml              # ISO codes and country metadata
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ html.py                     # Shared HTML parsing utilities
â”‚   â”œâ”€â”€ pdf.py
â”‚   â””â”€â”€ excel.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ health/
â”‚       â””â”€â”€ 2025/
â”‚           â””â”€â”€ 05/
â”‚               â””â”€â”€ 21/
â”‚                   â”œâ”€â”€ us.csv
â”‚                   â””â”€â”€ ja.csv
â”œâ”€â”€ topics/
â”‚   â”œâ”€â”€ health/
â”‚   â”‚   â”œâ”€â”€ us.yaml                 # Configuration for US health data
â”‚   â”‚   â””â”€â”€ ja.yaml                 # Configuration for Japan health data
â”‚   â””â”€â”€ education/
â”‚       â””â”€â”€ us.yaml                 # Configuration for US education data
â”œâ”€â”€ init.py                         # For first run
â”œâ”€â”€ crawl_all.py                    # Runs all crawlers daily
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ tests/
â”œâ”€â”€ .env
â””â”€â”€ .github/
    â””â”€â”€ workflows/
        â””â”€â”€ crawl.yml               # GitHub Action (runs daily)

```

---

## ğŸ“ `config/` Folder Details

### `countries.yaml`

```yaml
# ISO 3166-1 alpha-2 country codes used for naming files
US: United States
CN: China
KR: South Korea
JP: Japan
FR: France
IN: India
BR: Brazil
```

Use this file to:

- Ensure consistent country codes across scripts
- Prevent typos in filenames or crawler logic
- Drive dynamic discovery and validation

---

## ğŸ“„ YAML Configuration Format

Each data source is defined by a YAML file in the format `topics/<topic>/<countryCode>.yaml`.

### Example YAML Configuration

```yaml
# topics/health/us.yaml
source:
  type: html # Can be: html, pdf, excel, csv
  url: https://www.cdc.gov/nchs/fastats/health-expenditures.htm

extraction:
  selectors:
    - name: "Health Expenditure"
      selector: ".health-expenditure-value"
    - name: "Per Capita Spending"
      selector: ".per-capita-value"

  # For table extraction
  table_selector: "#health-data-table"
  header_row: 0

metadata:
  year: 2025
  source_name: "CDC"
  update_frequency: "annual"
```

### Source Types and Required Fields

| Source Type | Required Fields                                              | Description                        |
| ----------- | ------------------------------------------------------------ | ---------------------------------- |
| `html`      | `url`, `extraction.selectors` or `extraction.table_selector` | For crawling HTML pages            |
| `pdf`       | `url`                                                        | For extracting data from PDF files |
| `excel`     | `url`, `extraction.sheet_name`                               | For processing Excel spreadsheets  |
| `csv`       | `url`                                                        | For direct CSV downloads           |

---

## ğŸ“ Naming & Formatting Conventions

- **CSV File Naming**: `{countryCode}.csv`
- **YAML Config Naming**: `{countryCode}.yaml`
- **Code Format**: `black`
- **Linting**: `ruff`

---

## ğŸ¤ Contribution & Collaboration

### ğŸŒ± How to Contribute

1. Fork the repository
2. Add or update a configuration YAML file in `topics/<topic>/{countryCode}.yaml`
3. The crawler will automatically save output to `data/{topic}/{yyyy}/{mm}/{dd}/{countryCode}.csv`
4. Ensure the file is under 100MB
5. Open a PR and follow the review process

### ğŸŒ¿ Branching Strategy

- `main`: Stable, production-ready
- `dev`: Active development and testing

### âœ… PR Checklist

- [ ] YAML configuration file added or updated
- [ ] Output CSV placed correctly under `data/` when tested locally
- [ ] File size < 100MB
- [ ] `config/countries.yaml` updated (if new country)
- [ ] Tests included (if applicable)

### ğŸ Issues & Features

Open a GitHub Issue for:

- Bug reports
- Feature requests
- Country/topic suggestions

---

## ğŸ“„ Licensing & Contact Information

### ğŸ“œ License

**MIT License** â€“ Open source and free to reuse. See [`LICENSE`](./LICENSE).

### ğŸ“¬ Maintainer

[@iCyan](https://github.com/iCyanCorporation)

---

## ğŸ›£ï¸ Roadmap

- [x] Modular data source configuration via YAML
- [ ] Standardized CSV naming `{topic}_{countryCode}.csv`
- [x] Central `config/` folder for settings
- [x] GitHub Actions automation
- [ ] Web frontend for browsing datasets
- [ ] Auto-documentation of available datasets
- [ ] Language localization support
